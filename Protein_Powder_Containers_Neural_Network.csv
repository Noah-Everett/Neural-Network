Network Structure: nInputs: 10
                   nOutputs: 3
                   nLayers: 3
                   nNeurons: 4

Layer 0: Weights: 0.06848084366072715
                  -0.11510664311806484
                  -0.22951323803190266
                  -0.24173618223573545
                  0.1566351196001362
                  0.20637778863886086
                  0.05331788788358993
                  0.1147482804919992
                  0.021812495732711434
                  0.2175362118938841
                  0.15792677706076608
                  -0.24863074991492595
                  0.17870213829378467
                  -0.23320721234726782
                  0.11482772321497203
                  -0.1621721896987205
                  0.1815894611749433
                  0.020730610124545856
                  -0.10014405473130761
                  -0.038656389401170776
                  -0.23584016442726852
                  -0.18785836175021803
                  0.08531220734681516
                  0.07359475578712504
                  0.05769255574062693
                  -0.05816122286905828
                  0.2486049678946055
                  0.24041766938811504
                  0.09277099224034735
                  0.07522963813390815
                  0.09422336528547004
                  -0.05553928801044811
                  -0.1824517474887944
                  0.11074417009704085
                  0.012677161237862944
                  -0.09487906222052217
                  -0.0070823205841054615
                  0.19474391717450013
                  0.21702175797812484
                  -0.07110240164546489
         Biases: 0.03576491536488047
                 -0.08906530446202893
                 0.0471500150998484
                 -0.08104438724643337
         Activation: relu

Layer 1: Weights: -0.054190499735919384
                  0.19513717600239616
                  -0.13642120323331014
                  0.06159357234302121
                  -0.2079923282088076
                  0.1663220738266989
                  0.14354915374434168
                  -0.13031527850352392
                  0.1882421154053519
                  -0.22071598259740283
                  -0.08194146972716981
                  -0.17486026655258047
                  -0.024830316675356512
                  0.14816213514364712
                  -0.13467889550312628
                  -0.2239893494677952
         Biases: -0.0477240800892359
                 -0.15074347774537233
                 -0.20462347719043905
                 0.04016619299342533
         Activation: relu

Layer 2: Weights: -0.1006519335905387
                  0.08599743897817969
                  -0.15024227801589335
                  0.2210565552532489
                  -0.06744491587758572
                  -0.19725236021488524
                  0.06455407576985461
                  0.2135772765339337
                  -0.02981142264210801
                  0.22729524684536861
                  -5.209315617649146e-05
                  -0.037385687575462234
         Biases: 0.060106726007688904
                 0.24754825261766206
                 0.22447183746888266
         Activation: softmax

